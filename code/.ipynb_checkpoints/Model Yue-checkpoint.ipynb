{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape: (4559, 512)\n",
      "X2.shape: (4559, 30000)\n",
      "y1.shape: (4559,)\n",
      "y2.shape: (4559,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X1 = pd.read_csv('../data/features.csv', header=None).values\n",
    "X2 = pd.read_csv('../data/raw_images.csv', header=None).values\n",
    "y1 = pd.read_csv('../data/labels.csv', header=None).values.ravel().astype(int)\n",
    "y2 = pd.read_csv('../data/labels.csv', header=None).values.ravel().astype(int)\n",
    "\n",
    "print('X1.shape:', X1.shape)\n",
    "print('X2.shape:', X2.shape)\n",
    "print('y1.shape:', y1.shape)\n",
    "print('y2.shape:', y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test sizes: 3191 639 1368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = \\\n",
    "    train_test_split(X1, y1, test_size=0.3, random_state=123, shuffle=True, stratify=y1)\n",
    "\n",
    "X1_train_sub, X1_valid, y1_train_sub, y1_valid = \\\n",
    "    train_test_split(X1_train, y1_train, test_size=0.2, random_state=123, stratify=y1_train)\n",
    "\n",
    "print('Train/Valid/Test sizes:', y1_train.shape[0], y1_valid.shape[0], y1_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test sizes: 3191 639 1368\n"
     ]
    }
   ],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = \\\n",
    "    train_test_split(X2, y2, test_size=0.3, random_state=123, shuffle=True, stratify=y2)\n",
    "\n",
    "X2_train_sub, X2_valid, y2_train_sub, y2_valid = \\\n",
    "    train_test_split(X2_train, y2_train, test_size=0.2, random_state=123, stratify=y2_train)\n",
    "\n",
    "print('Train/Valid/Test sizes:', y2_train.shape[0], y2_valid.shape[0], y2_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7; total time=   2.8s\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7; total time=   2.7s\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7; total time=   2.8s\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7; total time=   3.1s\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7; total time=   3.0s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0; total time=   1.2s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0; total time=   1.3s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0; total time=   1.3s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0; total time=   1.2s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0; total time=   1.2s\n",
      "[CV] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7; total time=   1.2s\n",
      "[CV] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7; total time=   1.2s\n",
      "[CV] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7; total time=   1.1s\n",
      "[CV] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7; total time=   1.2s\n",
      "[CV] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7; total time=   1.4s\n",
      "[CV] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8; total time=   3.4s\n",
      "[CV] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8; total time=   3.1s\n",
      "[CV] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8; total time=   3.8s\n",
      "[CV] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8; total time=   3.6s\n",
      "[CV] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8; total time=   3.7s\n",
      "[22:49:59] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0; total time=   1.4s\n",
      "[22:50:01] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0; total time=   1.4s\n",
      "[22:50:02] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0; total time=   1.3s\n",
      "[22:50:03] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0; total time=   1.3s\n",
      "[22:50:05] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0; total time=   1.3s\n",
      "[22:50:06] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9; total time=   1.3s\n",
      "[22:50:07] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9; total time=   1.4s\n",
      "[22:50:09] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9; total time=   1.4s\n",
      "[22:50:10] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9; total time=   1.4s\n",
      "[22:50:12] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9; total time=   1.3s\n",
      "[22:50:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9; total time=   2.2s\n",
      "[22:50:15] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9; total time=   2.3s\n",
      "[22:50:17] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9; total time=   2.3s\n",
      "[22:50:20] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9; total time=   2.2s\n",
      "[22:50:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9; total time=   2.3s\n",
      "[22:50:24] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7; total time=   1.4s\n",
      "[22:50:26] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7; total time=   1.4s\n",
      "[22:50:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7; total time=   1.4s\n",
      "[22:50:28] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7; total time=   1.3s\n",
      "[22:50:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7; total time=   1.4s\n",
      "[22:50:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9; total time=   1.4s\n",
      "[22:50:32] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9; total time=   1.4s\n",
      "[22:50:34] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9; total time=   1.4s\n",
      "[22:50:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9; total time=   1.4s\n",
      "[22:50:37] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9; total time=   1.4s\n",
      "[22:50:38] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9; total time=   0.2s\n",
      "[22:50:38] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9; total time=   0.2s\n",
      "[22:50:38] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9; total time=   0.1s\n",
      "[22:50:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9; total time=   0.2s\n",
      "[22:50:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9; total time=   0.2s\n",
      "[22:50:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0; total time=   2.3s\n",
      "[22:50:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0; total time=   2.3s\n",
      "[22:50:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0; total time=   2.2s\n",
      "[22:50:46] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0; total time=   2.3s\n",
      "[22:50:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0; total time=   2.3s\n",
      "[CV] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0; total time=  17.9s\n",
      "[CV] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0; total time=  18.1s\n",
      "[CV] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0; total time=  16.9s\n",
      "[CV] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0; total time=  19.0s\n",
      "[CV] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0; total time=  17.1s\n",
      "[CV] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7; total time=   5.0s\n",
      "[CV] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7; total time=   5.3s\n",
      "[CV] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7; total time=   4.7s\n",
      "[CV] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7; total time=   4.6s\n",
      "[CV] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7; total time=   5.1s\n",
      "[CV] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0; total time=  11.7s\n",
      "[CV] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0; total time=  12.4s\n",
      "[CV] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0; total time=  12.2s\n",
      "[CV] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0; total time=  11.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0; total time=  11.6s\n",
      "[22:53:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7; total time=   0.3s\n",
      "[22:53:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7; total time=   0.3s\n",
      "[22:53:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7; total time=   0.2s\n",
      "[22:53:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7; total time=   0.2s\n",
      "[22:53:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7; total time=   0.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9197712923307872"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(random_state=123, use_label_encoder=False)\n",
    "\n",
    "params = {\n",
    "    'n_estimators':[30, 50, 100, 300, 500],\n",
    "    'min_child_weight':[4,5], \n",
    "    \"reg_lambda\": scipy.stats.uniform(1e-8, 1.0),\n",
    "    \"alpha\": scipy.stats.uniform(1e-8, 1.0),\n",
    "    'gamma':[i/10.0 for i in range(3,6)],  \n",
    "    'subsample':[i/10.0 for i in range(6,11)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,11)], \n",
    "    'max_depth': [2,3,4,6,7],\n",
    "    'objective': ['reg:squarederror', 'reg:tweedie'],\n",
    "    'booster': ['gbtree', 'gblinear'],\n",
    "    'eval_metric': ['rmse'],\n",
    "    'eta': [i/10.0 for i in range(3,6)],\n",
    "}\n",
    "\n",
    "search1 = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search1.fit(X1_train, y1_train)\n",
    "\n",
    "search1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.6917018087001772,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bytree': 0.7,\n",
       " 'eta': 0.3,\n",
       " 'eval_metric': 'rmse',\n",
       " 'gamma': 0.3,\n",
       " 'max_depth': 7,\n",
       " 'min_child_weight': 4,\n",
       " 'n_estimators': 100,\n",
       " 'objective': 'reg:tweedie',\n",
       " 'reg_lambda': 0.029423743551983135,\n",
       " 'subsample': 0.7}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  99.94%\n",
      "Test Accuracy:  91.74%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search1.best_estimator_.score(X1_train, y1_train)*100: 0.2f}%\") \n",
    "print(f\"Test Accuracy: {search1.best_estimator_.score(X1_test, y1_test)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9170036919863896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 score: {f1_score(y1_test, search1.best_estimator_.predict(X1_test), average='weighted')}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV 1/5; 1/15] START alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7\n",
      "[CV 1/5; 1/15] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7;, score=0.930 total time= 3.3min\n",
      "[CV 2/5; 1/15] START alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7\n",
      "[CV 2/5; 1/15] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7;, score=0.936 total time= 3.0min\n",
      "[CV 3/5; 1/15] START alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7\n",
      "[CV 3/5; 1/15] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7;, score=0.951 total time= 3.0min\n",
      "[CV 4/5; 1/15] START alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7\n",
      "[CV 4/5; 1/15] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7;, score=0.937 total time= 3.0min\n",
      "[CV 5/5; 1/15] START alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7\n",
      "[CV 5/5; 1/15] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7;, score=0.953 total time= 3.0min\n",
      "[CV 1/5; 2/15] START alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0\n",
      "[CV 1/5; 2/15] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0;, score=0.933 total time= 1.1min\n",
      "[CV 2/5; 2/15] START alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0\n",
      "[CV 2/5; 2/15] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0;, score=0.937 total time= 1.1min\n",
      "[CV 3/5; 2/15] START alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0\n",
      "[CV 3/5; 2/15] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0;, score=0.945 total time= 1.1min\n",
      "[CV 4/5; 2/15] START alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0\n",
      "[CV 4/5; 2/15] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0;, score=0.926 total time= 1.1min\n",
      "[CV 5/5; 2/15] START alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0\n",
      "[CV 5/5; 2/15] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0;, score=0.953 total time= 1.1min\n",
      "[CV 1/5; 3/15] START alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7\n",
      "[CV 1/5; 3/15] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7;, score=0.933 total time= 1.2min\n",
      "[CV 2/5; 3/15] START alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7\n",
      "[CV 2/5; 3/15] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7;, score=0.937 total time= 1.2min\n",
      "[CV 3/5; 3/15] START alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7\n",
      "[CV 3/5; 3/15] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7;, score=0.950 total time= 1.2min\n",
      "[CV 4/5; 3/15] START alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7\n",
      "[CV 4/5; 3/15] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7;, score=0.931 total time= 1.2min\n",
      "[CV 5/5; 3/15] START alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7\n",
      "[CV 5/5; 3/15] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7;, score=0.959 total time= 1.2min\n",
      "[CV 1/5; 4/15] START alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8\n",
      "[CV 1/5; 4/15] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8;, score=0.937 total time= 2.9min\n",
      "[CV 2/5; 4/15] START alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 4/15] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8;, score=0.937 total time= 2.8min\n",
      "[CV 3/5; 4/15] START alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8\n",
      "[CV 3/5; 4/15] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8;, score=0.948 total time= 2.9min\n",
      "[CV 4/5; 4/15] START alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8\n",
      "[CV 4/5; 4/15] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8;, score=0.928 total time= 2.8min\n",
      "[CV 5/5; 4/15] START alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8\n",
      "[CV 5/5; 4/15] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8;, score=0.950 total time= 3.0min\n",
      "[CV 1/5; 5/15] START alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0\n",
      "[00:34:09] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 5/15] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0;, score=0.928 total time= 2.3min\n",
      "[CV 2/5; 5/15] START alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0\n",
      "[00:36:28] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 5/15] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0;, score=0.915 total time= 2.2min\n",
      "[CV 3/5; 5/15] START alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0\n",
      "[00:38:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 5/15] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0;, score=0.936 total time= 2.1min\n",
      "[CV 4/5; 5/15] START alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0\n",
      "[00:40:42] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 5/15] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0;, score=0.923 total time= 2.2min\n",
      "[CV 5/5; 5/15] START alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0\n",
      "[00:42:53] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 5/15] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0;, score=0.931 total time= 2.2min\n",
      "[CV 1/5; 6/15] START alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9\n",
      "[00:45:06] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 6/15] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9;, score=0.942 total time= 2.5min\n",
      "[CV 2/5; 6/15] START alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9\n",
      "[00:47:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 6/15] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9;, score=0.925 total time= 2.5min\n",
      "[CV 3/5; 6/15] START alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9\n",
      "[00:50:07] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 6/15] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9;, score=0.940 total time= 2.4min\n",
      "[CV 4/5; 6/15] START alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9\n",
      "[00:52:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 6/15] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9;, score=0.928 total time= 2.4min\n",
      "[CV 5/5; 6/15] START alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9\n",
      "[00:54:57] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 6/15] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9;, score=0.931 total time= 2.4min\n",
      "[CV 1/5; 7/15] START alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9\n",
      "[00:57:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 7/15] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9;, score=0.944 total time= 4.6min\n",
      "[CV 2/5; 7/15] START alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9\n",
      "[01:01:55] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 7/15] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9;, score=0.925 total time= 4.6min\n",
      "[CV 3/5; 7/15] START alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9\n",
      "[01:06:33] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 7/15] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9;, score=0.936 total time= 4.6min\n",
      "[CV 4/5; 7/15] START alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9\n",
      "[01:11:12] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 7/15] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9;, score=0.929 total time= 4.6min\n",
      "[CV 5/5; 7/15] START alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9\n",
      "[01:15:46] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 7/15] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9;, score=0.929 total time= 4.5min\n",
      "[CV 1/5; 8/15] START alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:20:16] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 8/15] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7;, score=0.934 total time= 2.2min\n",
      "[CV 2/5; 8/15] START alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7\n",
      "[01:22:24] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 8/15] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7;, score=0.918 total time= 2.2min\n",
      "[CV 3/5; 8/15] START alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7\n",
      "[01:24:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 8/15] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7;, score=0.936 total time= 2.1min\n",
      "[CV 4/5; 8/15] START alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7\n",
      "[01:26:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 8/15] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7;, score=0.925 total time= 2.1min\n",
      "[CV 5/5; 8/15] START alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7\n",
      "[01:28:51] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 8/15] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7;, score=0.931 total time= 2.1min\n",
      "[CV 1/5; 9/15] START alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9\n",
      "[01:31:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 9/15] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9;, score=0.950 total time= 2.8min\n",
      "[CV 2/5; 9/15] START alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9\n",
      "[01:33:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 9/15] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9;, score=0.925 total time= 2.9min\n",
      "[CV 3/5; 9/15] START alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9\n",
      "[01:36:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 9/15] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9;, score=0.940 total time= 2.8min\n",
      "[CV 4/5; 9/15] START alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9\n",
      "[01:39:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 9/15] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9;, score=0.934 total time= 2.8min\n",
      "[CV 5/5; 9/15] START alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9\n",
      "[01:42:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 9/15] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9;, score=0.937 total time= 2.8min\n",
      "[CV 1/5; 10/15] START alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9\n",
      "[01:45:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 10/15] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9;, score=0.953 total time=  11.9s\n",
      "[CV 2/5; 10/15] START alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9\n",
      "[01:45:25] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 10/15] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9;, score=0.929 total time=  11.9s\n",
      "[CV 3/5; 10/15] START alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9\n",
      "[01:45:37] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 10/15] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9;, score=0.945 total time=  11.8s\n",
      "[CV 4/5; 10/15] START alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9\n",
      "[01:45:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 10/15] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9;, score=0.936 total time=  11.8s\n",
      "[CV 5/5; 10/15] START alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9\n",
      "[01:46:01] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 10/15] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9;, score=0.947 total time=  11.8s\n",
      "[CV 1/5; 11/15] START alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0\n",
      "[01:46:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 11/15] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0;, score=0.931 total time= 3.5min\n",
      "[CV 2/5; 11/15] START alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0\n",
      "[01:49:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 11/15] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0;, score=0.920 total time= 4.3min\n",
      "[CV 3/5; 11/15] START alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:54:01] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 11/15] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0;, score=0.936 total time= 3.9min\n",
      "[CV 4/5; 11/15] START alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0\n",
      "[01:57:53] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 11/15] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0;, score=0.925 total time= 3.6min\n",
      "[CV 5/5; 11/15] START alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0\n",
      "[02:01:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 11/15] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0;, score=0.929 total time= 3.6min\n",
      "[CV 1/5; 12/15] START alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0\n",
      "[CV 1/5; 12/15] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0;, score=0.933 total time=11.8min\n",
      "[CV 2/5; 12/15] START alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0\n",
      "[CV 2/5; 12/15] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0;, score=0.936 total time=12.1min\n",
      "[CV 3/5; 12/15] START alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0\n",
      "[CV 3/5; 12/15] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0;, score=0.950 total time=11.7min\n",
      "[CV 4/5; 12/15] START alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0\n",
      "[CV 4/5; 12/15] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0;, score=0.939 total time=11.6min\n",
      "[CV 5/5; 12/15] START alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0\n",
      "[CV 5/5; 12/15] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0;, score=0.956 total time=12.0min\n",
      "[CV 1/5; 13/15] START alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7\n",
      "[CV 1/5; 13/15] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7;, score=0.937 total time= 3.5min\n",
      "[CV 2/5; 13/15] START alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7\n",
      "[CV 2/5; 13/15] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7;, score=0.929 total time= 3.4min\n",
      "[CV 3/5; 13/15] START alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7\n",
      "[CV 3/5; 13/15] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7;, score=0.948 total time= 3.4min\n",
      "[CV 4/5; 13/15] START alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7\n",
      "[CV 4/5; 13/15] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7;, score=0.937 total time= 3.3min\n",
      "[CV 5/5; 13/15] START alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7\n",
      "[CV 5/5; 13/15] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7;, score=0.959 total time= 3.5min\n",
      "[CV 1/5; 14/15] START alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 14/15] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0;, score=0.936 total time= 8.1min\n",
      "[CV 2/5; 14/15] START alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0\n",
      "[CV 2/5; 14/15] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0;, score=0.933 total time= 8.3min\n",
      "[CV 3/5; 14/15] START alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0\n",
      "[CV 3/5; 14/15] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0;, score=0.950 total time= 8.4min\n",
      "[CV 4/5; 14/15] START alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0\n",
      "[CV 4/5; 14/15] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0;, score=0.934 total time= 8.0min\n",
      "[CV 5/5; 14/15] START alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0\n",
      "[CV 5/5; 14/15] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0;, score=0.950 total time= 8.1min\n",
      "[CV 1/5; 15/15] START alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7\n",
      "[04:02:28] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 15/15] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7;, score=0.948 total time=  16.6s\n",
      "[CV 2/5; 15/15] START alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7\n",
      "[04:02:45] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 15/15] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7;, score=0.929 total time=  16.7s\n",
      "[CV 3/5; 15/15] START alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7\n",
      "[04:03:02] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 15/15] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7;, score=0.947 total time=  16.8s\n",
      "[CV 4/5; 15/15] START alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7\n",
      "[04:03:19] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 15/15] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7;, score=0.934 total time=  16.7s\n",
      "[CV 5/5; 15/15] START alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7\n",
      "[04:03:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 15/15] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7;, score=0.940 total time=  16.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94265432371309"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators':[30, 50, 100, 300, 500],\n",
    "    'min_child_weight':[4,5], \n",
    "    \"reg_lambda\": scipy.stats.uniform(1e-8, 1.0),\n",
    "    \"alpha\": scipy.stats.uniform(1e-8, 1.0),\n",
    "    'gamma':[i/10.0 for i in range(3,6)],  \n",
    "    'subsample':[i/10.0 for i in range(6,11)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,11)], \n",
    "    'max_depth': [2,3,4,6,7],\n",
    "    'objective': ['reg:squarederror', 'reg:tweedie'],\n",
    "    'booster': ['gbtree', 'gblinear'],\n",
    "    'eval_metric': ['rmse'],\n",
    "    'eta': [i/10.0 for i in range(3,6)],\n",
    "}\n",
    "\n",
    "search2 = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=5,\n",
    "    verbose=10,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search2.fit(X2_train, y2_train)\n",
    "\n",
    "search2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.3929444207680876,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bytree': 1.0,\n",
       " 'eta': 0.3,\n",
       " 'eval_metric': 'rmse',\n",
       " 'gamma': 0.5,\n",
       " 'max_depth': 2,\n",
       " 'min_child_weight': 4,\n",
       " 'n_estimators': 500,\n",
       " 'objective': 'reg:squarederror',\n",
       " 'reg_lambda': 0.0576480249714419,\n",
       " 'subsample': 1.0}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  100.00%\n",
      "Test Accuracy:  94.88%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search2.best_estimator_.score(X2_train, y2_train)*100: 0.2f}%\") \n",
    "print(f\"Test Accuracy: {search2.best_estimator_.score(X2_test, y2_test)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9487152602563024\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 score: {f1_score(y2_test, search2.best_estimator_.predict(X2_test), average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[431,  10,   2],\n",
       "       [ 19, 422,   3],\n",
       "       [ 18,  18, 445]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "xgb_cm_true = contingency_matrix(search2.best_estimator_.predict(X2_test), y2_test)\n",
    "xgb_cm_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plot_confusion_matrix(conf_mat=xgb_cm_true,\n",
    "                                show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True,\n",
    "                                figsize=(3, 3))\n",
    "plt.savefig(\"xgb_cm.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.759635579937304"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=123, max_iter = 1000)\n",
    "\n",
    "params = {\n",
    "    \"C\": scipy.stats.expon(scale=.01),\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"warm_start\": [True,False]\n",
    "}\n",
    "\n",
    "search3 = RandomizedSearchCV(\n",
    "    estimator=lr,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=10,\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search3.fit(X1_train, y1_train)\n",
    "\n",
    "search3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.03950982068814718, 'fit_intercept': True, 'warm_start': False}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  76.68%\n",
      "Test Accuracy:  78.22%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search3.best_estimator_.score(X1_train, y1_train)*100: 0.2f}%\") \n",
    "print(f\"Test Accuracy: {search3.best_estimator_.score(X1_test, y1_test)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7691277689909498\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 score: {f1_score(y1_test, search3.best_estimator_.predict(X1_test), average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time= 1.7min\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time= 1.9min\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time= 1.7min\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time= 1.5min\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time= 1.8min\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time= 1.7min\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time= 1.7min\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time= 1.7min\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time= 1.4min\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time= 1.7min\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time= 1.7min\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time= 1.3min\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time= 1.7min\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time= 1.5min\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time= 1.5min\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time= 1.5min\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time= 1.7min\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time= 1.7min\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time= 1.4min\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time= 1.5min\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time= 1.5min\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9288631825785784"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search4 = RandomizedSearchCV(\n",
    "    estimator=lr,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search4.fit(X2_train, y2_train)\n",
    "\n",
    "search4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.002572840801170508, 'fit_intercept': True, 'warm_start': False}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  100.00%\n",
      "Test Accuracy:  93.20%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search4.best_estimator_.score(X2_train, y2_train)*100: 0.2f}%\") \n",
    "print(f\"Test Accuracy: {search4.best_estimator_.score(X2_test, y2_test)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.931976655005432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 score: {f1_score(y2_test, search4.best_estimator_.predict(X2_test), average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[430,  17,   8],\n",
       "       [ 25, 415,  12],\n",
       "       [ 13,  18, 430]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "lr_cm_true = contingency_matrix(search4.best_estimator_.predict(X2_test), y2_test)\n",
    "lr_cm_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plot_confusion_matrix(conf_mat=lr_cm_true,\n",
    "                                show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True,\n",
    "                                figsize=(3, 3))\n",
    "plt.savefig(\"lr_cm.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree + Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8691232109282069"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "params =  {\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'max_depth': [5, 7, 9, 11, None],\n",
    "    'criterion':['entropy', 'gini']\n",
    "}\n",
    "\n",
    "search5 = RandomizedSearchCV(\n",
    "    estimator=tree,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search5.fit(X1_train_sub, y1_train_sub)\n",
    "\n",
    "search5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 3, 'max_depth': 5, 'criterion': 'gini'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search5.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  91.77%\n",
      "Valid Accuracy:  87.32%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search5.best_estimator_.score(X1_train_sub, y1_train_sub)*100: 0.2f}%\") \n",
    "print(f\"Valid Accuracy: {search5.best_estimator_.score(X1_valid, y1_valid)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 93.06%\n",
      "Valid Accuracy: 88.58%\n",
      "Test Accuracy: 89.18%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag1 = BaggingClassifier(base_estimator=search5.best_estimator_, \n",
    "                        n_estimators=150, \n",
    "                        oob_score=True, \n",
    "                        bootstrap=True,\n",
    "                        bootstrap_features=False, \n",
    "                        n_jobs=1, \n",
    "                        random_state=123)\n",
    "\n",
    "bag1.fit(X1_train_sub, y1_train_sub)\n",
    "print(f\"Train Accuracy: {bag1.score(X1_train_sub, y1_train_sub)*100:0.2f}%\")\n",
    "print(f\"Valid Accuracy: {bag1.score(X1_valid, y1_valid)*100:0.2f}%\")\n",
    "print(f\"Test Accuracy: {bag1.score(X1_test, y1_test)*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8912916961773778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 score: {f1_score(y1_test, bag1.predict(X1_test), average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "search6 = DecisionTreeClassifier(max_depth=4, \n",
    "                                 min_impurity_decrease=0.011493972900871952,\n",
    "                                 min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 91.77%\n",
      "Valid Accuracy: 90.30%\n",
      "Test Accuracy: 91.23%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag2 = BaggingClassifier(base_estimator=search6, \n",
    "                        n_estimators=50, \n",
    "                        oob_score=True, \n",
    "                        bootstrap=True,\n",
    "                        bootstrap_features=False, \n",
    "                        n_jobs=1, \n",
    "                        random_state=123)\n",
    "\n",
    "bag2.fit(X2_train_sub, y2_train_sub)\n",
    "print(f\"Train Accuracy: {bag2.score(X2_train_sub, y2_train_sub)*100:0.2f}%\")\n",
    "print(f\"Valid Accuracy: {bag2.score(X2_valid, y2_valid)*100:0.2f}%\")\n",
    "print(f\"Test Accuracy: {bag2.score(X2_test, y2_test)*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9121410382503217\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 score: {f1_score(y2_test, bag2.predict(X2_test), average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[418,  22,   8],\n",
       "       [ 22, 399,  11],\n",
       "       [ 28,  29, 431]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "bag_cm_true = contingency_matrix(bag2.predict(X2_test), y2_test)\n",
    "bag_cm_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plot_confusion_matrix(conf_mat=bag_cm_true,\n",
    "                                show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True,\n",
    "                                figsize=(3, 3))\n",
    "plt.savefig(\"bag_cm.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McNemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest2 = RandomForestClassifier(n_estimators=100,\n",
    "                                random_state=123)\n",
    "\n",
    "forest2.fit(X2_train, y2_train)\n",
    "    \n",
    "print(\"Training Accuracy: %0.4f\" % forest2.score(X2_train, y2_train))\n",
    "print(\"Validation Accuracy: %0.4f\" % forest2.score(X2_valid, y2_valid))\n",
    "print(\"Test Accuracy: %0.4f\" % forest2.score(X2_test, y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1277   15]\n",
      " [  21   55]]\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y2_test\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_forest = forest2.predict(X2_test)\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_xgb = search2.best_estimator_.predict(X2_test)\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_forest, \n",
    "                   y_model2=y_xgb)\n",
    "\n",
    "print(tb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi-squared: 0.6944444444444444\n",
      "p-value: 0.40465676192728617\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import mcnemar\n",
    "\n",
    "chi2, p = mcnemar(ary=tb, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our own photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.3929444207680876, base_score=0.5, booster='gbtree',\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1.0,\n",
       "              eta=0.3, eval_metric='rmse', gamma=0.5, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
       "              min_child_weight=4, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=500, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=123,\n",
       "              reg_alpha=0.392944425, reg_lambda=0.0576480249714419,\n",
       "              scale_pos_weight=None, subsample=1.0, tree_method='exact',\n",
       "              use_label_encoder=False, validate_parameters=1, ...)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(random_state=123, \n",
    "                    use_label_encoder=False,\n",
    "                    alpha=0.3929444207680876,\n",
    "                    booster='gbtree',\n",
    "                    colsample_bytree=1.0,\n",
    "                    eta=0.3,\n",
    "                    eval_metric='rmse',\n",
    "                    gamma=0.5,\n",
    "                    max_depth=2,\n",
    "                    min_child_weight=4,\n",
    "                    n_estimators=500,\n",
    "                    objective='reg:squarederror',\n",
    "                    reg_lambda=0.0576480249714419,\n",
    "                    subsample=1.0)\n",
    "xgb.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = pd.read_csv('../data/incorrect_data.csv', header=None).values\n",
    "image2 = pd.read_csv('../data/test_data.csv', header=None).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = image1.transpose()\n",
    "image2 = image2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.predict(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.predict(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "image3 = pd.read_csv('mask.csv', header=None).values\n",
    "image3 = image3.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.predict(image3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9488304093567251"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.score(X2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>29990</th>\n",
       "      <th>29991</th>\n",
       "      <th>29992</th>\n",
       "      <th>29993</th>\n",
       "      <th>29994</th>\n",
       "      <th>29995</th>\n",
       "      <th>29996</th>\n",
       "      <th>29997</th>\n",
       "      <th>29998</th>\n",
       "      <th>29999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>183.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>77.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>148.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4062</th>\n",
       "      <td>196.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>135.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>199.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300</th>\n",
       "      <td>92.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>250.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>11.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>217.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>72.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3959</th>\n",
       "      <td>99.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>83.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3191 rows  30000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2      3      4      5      6      7      8      9      \\\n",
       "218   183.0  174.0  167.0  185.0  178.0  173.0  171.0  162.0  154.0  163.0   \n",
       "2148   77.0   95.0  106.0   55.0   59.0   66.0   47.0   52.0   53.0   33.0   \n",
       "4062  196.0  190.0  190.0   69.0   56.0   61.0   60.0   46.0   45.0   49.0   \n",
       "714   199.0  112.0   59.0  197.0  110.0   56.0  198.0  112.0   55.0  195.0   \n",
       "3300   92.0   81.0   81.0   98.0   88.0   85.0  118.0  108.0  104.0  120.0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1000   11.0   35.0   27.0    3.0   27.0   14.0    2.0   20.0    4.0    1.0   \n",
       "2782  217.0  206.0  204.0  214.0  200.0  197.0  204.0  189.0  186.0  200.0   \n",
       "74     72.0   67.0   84.0   78.0   71.0   90.0   66.0   51.0   71.0   80.0   \n",
       "3959   99.0   72.0   58.0  106.0   79.0   67.0  110.0   83.0   72.0  107.0   \n",
       "3256   83.0   83.0   70.0   71.0   66.0   59.0   86.0   76.0   68.0   77.0   \n",
       "\n",
       "      ...  29990  29991  29992  29993  29994  29995  29996  29997  29998  \\\n",
       "218   ...   16.0   17.0   16.0   15.0   18.0   18.0   17.0   17.0   17.0   \n",
       "2148  ...  148.0  151.0  123.0  134.0  177.0  128.0  139.0  198.0  152.0   \n",
       "4062  ...  135.0  121.0   68.0   64.0   81.0   47.0   34.0   66.0   51.0   \n",
       "714   ...   99.0  223.0  156.0  101.0  243.0  178.0  124.0  248.0  185.0   \n",
       "3300  ...  250.0  117.0  163.0  244.0  116.0  157.0  230.0   97.0  138.0   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1000  ...    0.0    2.0   19.0    3.0    1.0   17.0    1.0    1.0   17.0   \n",
       "2782  ...   33.0   30.0   28.0   33.0   29.0   27.0   32.0   29.0   27.0   \n",
       "74    ...   30.0   56.0   22.0   31.0   53.0   19.0   30.0   67.0   28.0   \n",
       "3959  ...   31.0   23.0   25.0   33.0   25.0   28.0   34.0   25.0   28.0   \n",
       "3256  ...    7.0   15.0    9.0    2.0   15.0   11.0    2.0   18.0   12.0   \n",
       "\n",
       "      29999  \n",
       "218    17.0  \n",
       "2148  163.0  \n",
       "4062   40.0  \n",
       "714   134.0  \n",
       "3300  217.0  \n",
       "...     ...  \n",
       "1000    0.0  \n",
       "2782   32.0  \n",
       "74     33.0  \n",
       "3959   35.0  \n",
       "3256    5.0  \n",
       "\n",
       "[3191 rows x 30000 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_train_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
