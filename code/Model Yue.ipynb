{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape: (4559, 512)\n",
      "X2.shape: (4559, 30000)\n",
      "y1.shape: (4559,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X1 = pd.read_csv('../data/features.csv', header=None).values\n",
    "X2 = pd.read_csv('../data/raw_images.csv', header=None).values\n",
    "y1 = pd.read_csv('../data/labels.csv', header=None).values.ravel().astype(int)\n",
    "\n",
    "print('X1.shape:', X1.shape)\n",
    "print('X2.shape:', X2.shape)\n",
    "print('y1.shape:', y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test sizes: 3191 639 1368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = \\\n",
    "    train_test_split(X1, y1, test_size=0.3, random_state=123, shuffle=True, stratify=y1)\n",
    "\n",
    "X1_train_sub, X1_valid, y1_train_sub, y1_valid = \\\n",
    "    train_test_split(X1_train, y1_train, test_size=0.2, random_state=123, stratify=y1_train)\n",
    "\n",
    "print('Train/Valid/Test sizes:', y1_train.shape[0], y1_valid.shape[0], y1_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test sizes: 3191 639 1368\n"
     ]
    }
   ],
   "source": [
    "X2_train, X2_test, y1_train, y1_test = \\\n",
    "    train_test_split(X2, y1, test_size=0.3, random_state=123, shuffle=True, stratify=y1)\n",
    "\n",
    "X2_train_sub, X2_valid, y1_train_sub, y1_valid = \\\n",
    "    train_test_split(X2_train, y1_train, test_size=0.2, random_state=123, stratify=y1_train)\n",
    "\n",
    "print('Train/Valid/Test sizes:', y1_train.shape[0], y1_valid.shape[0], y1_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7; total time=  14.8s\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7; total time=  14.9s\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7; total time=  15.5s\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7; total time=  15.1s\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7; total time=  15.0s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6; total time=  12.0s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6; total time=  12.1s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6; total time=  12.0s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6; total time=  12.0s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6; total time=  11.9s\n",
      "[19:37:21] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6; total time=   0.1s\n",
      "[19:37:21] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6; total time=   0.1s\n",
      "[19:37:21] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6; total time=   0.1s\n",
      "[19:37:21] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6; total time=   0.1s\n",
      "[19:37:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6; total time=   0.1s\n",
      "[19:37:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6; total time=   0.2s\n",
      "[19:37:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6; total time=   0.2s\n",
      "[19:37:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6; total time=   0.2s\n",
      "[19:37:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6; total time=   0.2s\n",
      "[19:37:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6; total time=   0.2s\n",
      "[19:37:23] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0; total time=   0.9s\n",
      "[19:37:23] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0; total time=   0.9s\n",
      "[19:37:24] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0; total time=   0.9s\n",
      "[19:37:25] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0; total time=   0.9s\n",
      "[19:37:26] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0; total time=   0.9s\n",
      "[CV] END alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7; total time=  12.6s\n",
      "[CV] END alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7; total time=  12.3s\n",
      "[CV] END alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7; total time=  12.5s\n",
      "[CV] END alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7; total time=  12.4s\n",
      "[CV] END alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7; total time=  12.5s\n",
      "[19:38:29] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9; total time=   0.2s\n",
      "[19:38:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9; total time=   0.2s\n",
      "[19:38:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9; total time=   0.2s\n",
      "[19:38:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9; total time=   0.2s\n",
      "[19:38:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9; total time=   0.2s\n",
      "[19:38:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7; total time=   0.2s\n",
      "[19:38:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7; total time=   0.2s\n",
      "[19:38:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7; total time=   0.2s\n",
      "[19:38:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7; total time=   0.2s\n",
      "[19:38:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7; total time=   0.2s\n",
      "[19:38:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9; total time=   1.5s\n",
      "[19:38:33] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9; total time=   1.4s\n",
      "[19:38:34] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9; total time=   1.5s\n",
      "[19:38:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9; total time=   1.5s\n",
      "[19:38:37] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9; total time=   1.5s\n",
      "[19:38:38] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9; total time=   0.1s\n",
      "[19:38:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9; total time=   0.1s\n",
      "[19:38:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9; total time=   0.1s\n",
      "[19:38:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9; total time=   0.1s\n",
      "[19:38:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9; total time=   0.1s\n",
      "[19:38:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0; total time=   1.5s\n",
      "[19:38:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0; total time=   1.5s\n",
      "[19:38:42] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0; total time=   1.5s\n",
      "[19:38:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0; total time=   1.6s\n",
      "[19:38:45] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0; total time=   1.5s\n",
      "[19:38:47] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6; total time=   1.5s\n",
      "[19:38:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6; total time=   1.4s\n",
      "[19:38:50] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6; total time=   1.5s\n",
      "[19:38:51] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6; total time=   1.5s\n",
      "[19:38:53] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6; total time=   1.5s\n",
      "[CV] END alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6; total time=   0.8s\n",
      "[CV] END alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6; total time=   0.8s\n",
      "[CV] END alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6; total time=   0.8s\n",
      "[CV] END alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6; total time=   0.8s\n",
      "[CV] END alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6; total time=   0.8s\n",
      "[CV] END alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7; total time=   3.1s\n",
      "[CV] END alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7; total time=   3.0s\n",
      "[CV] END alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7; total time=   3.0s\n",
      "[CV] END alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7; total time=   2.9s\n",
      "[CV] END alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7; total time=   3.1s\n",
      "[19:39:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7; total time=   0.3s\n",
      "[19:39:14] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7; total time=   0.3s\n",
      "[19:39:14] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7; total time=   0.3s\n",
      "[19:39:14] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7; total time=   0.3s\n",
      "[19:39:15] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7; total time=   0.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9229060885690318"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(random_state=123, use_label_encoder=False)\n",
    "\n",
    "params = {\n",
    "    'n_estimators':[30, 50, 100, 300, 500],\n",
    "    'min_child_weight':[4,5], \n",
    "    \"lambda\": scipy.stats.uniform(1e-8, 1.0),\n",
    "    \"alpha\": scipy.stats.uniform(1e-8, 1.0),\n",
    "    'gamma':[i/10.0 for i in range(3,6)],  \n",
    "    'subsample':[i/10.0 for i in range(6,11)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,11)], \n",
    "    'max_depth': [2,3,4,6,7],\n",
    "    'objective': ['reg:squarederror', 'reg:tweedie'],\n",
    "    'booster': ['gbtree', 'gblinear'],\n",
    "    'eval_metric': ['rmse'],\n",
    "    'eta': [i/10.0 for i in range(3,6)],\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search.fit(X1_train, y1_train)\n",
    "\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.6848297485848633,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bytree': 0.7,\n",
       " 'eta': 0.3,\n",
       " 'eval_metric': 'rmse',\n",
       " 'gamma': 0.5,\n",
       " 'lambda': 0.6273170193376167,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 5,\n",
       " 'n_estimators': 500,\n",
       " 'objective': 'reg:squarederror',\n",
       " 'subsample': 0.6}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  99.94%\n",
      "Test Accuracy:  91.23%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search.best_estimator_.score(X1_train, y1_train)*100: 0.2f}%\") \n",
    "print(f\"Test Accuracy: {search.best_estimator_.score(X1_test, y1_test)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv_acc = cross_val_score(estimator=XGBClassifier(random_state=123, \n",
    "                                                 use_label_encoder=False,\n",
    "                                                 alpha=0.6848297485848633,\n",
    "                                                 booster='gbtree',\n",
    "                                                 colsample_bytree=0.7,\n",
    "                                                 eta=0.3,\n",
    "                                                 eval_metric='rmse',\n",
    "                                                 gamma=0.5,\n",
    "                                                 'lambda'=0.6273170193376167,\n",
    "                                                 max_depth= 3,\n",
    "                                                 min_child_weight=5,\n",
    "                                                 n_estimators=500,\n",
    "                                                 objective='reg:squarederror',\n",
    "                                                 subsample=0.6),\n",
    "                         X=X1,\n",
    "                         y=y1,\n",
    "                         cv=StratifiedKFold(n_splits=10, random_state=123, shuffle=True),\n",
    "                         n_jobs=-1)\n",
    "\n",
    "print('Kfold Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV 1/5; 1/15] START alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7\n",
      "[CV 1/5; 1/15] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7;, score=0.933 total time=11.3min\n",
      "[CV 2/5; 1/15] START alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7\n",
      "[CV 2/5; 1/15] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7;, score=0.937 total time=11.7min\n",
      "[CV 3/5; 1/15] START alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7\n",
      "[CV 3/5; 1/15] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7;, score=0.948 total time=11.3min\n",
      "[CV 4/5; 1/15] START alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7\n",
      "[CV 4/5; 1/15] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7;, score=0.931 total time=11.1min\n",
      "[CV 5/5; 1/15] START alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7\n",
      "[CV 5/5; 1/15] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.5513147790828913, max_depth=6, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=0.7;, score=0.959 total time=11.3min\n",
      "[CV 1/5; 2/15] START alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6\n",
      "[CV 1/5; 2/15] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6;, score=0.942 total time=11.5min\n",
      "[CV 2/5; 2/15] START alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6\n",
      "[CV 2/5; 2/15] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6;, score=0.929 total time=10.7min\n",
      "[CV 3/5; 2/15] START alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6\n",
      "[CV 3/5; 2/15] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6;, score=0.947 total time=10.8min\n",
      "[CV 4/5; 2/15] START alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6\n",
      "[CV 4/5; 2/15] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6;, score=0.942 total time=11.3min\n",
      "[CV 5/5; 2/15] START alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6\n",
      "[CV 5/5; 2/15] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.6273170193376167, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.6;, score=0.961 total time=10.9min\n",
      "[CV 1/5; 3/15] START alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6\n",
      "[16:54:42] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 3/15] END alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6;, score=0.934 total time=   8.3s\n",
      "[CV 2/5; 3/15] START alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6\n",
      "[16:54:51] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 3/15] END alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6;, score=0.914 total time=   8.1s\n",
      "[CV 3/5; 3/15] START alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6\n",
      "[16:54:59] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 3/15] END alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6;, score=0.939 total time=   8.1s\n",
      "[CV 4/5; 3/15] START alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:55:07] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 3/15] END alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6;, score=0.928 total time=   8.1s\n",
      "[CV 5/5; 3/15] START alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6\n",
      "[16:55:15] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 3/15] END alpha=0.7379954157320358, booster=gblinear, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.5315513838418384, max_depth=7, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.6;, score=0.931 total time=   8.2s\n",
      "[CV 1/5; 4/15] START alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6\n",
      "[16:55:24] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 4/15] END alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6;, score=0.937 total time=  13.0s\n",
      "[CV 2/5; 4/15] START alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6\n",
      "[16:55:37] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 4/15] END alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6;, score=0.928 total time=  13.1s\n",
      "[CV 3/5; 4/15] START alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6\n",
      "[16:55:50] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 4/15] END alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6;, score=0.945 total time=  13.1s\n",
      "[CV 4/5; 4/15] START alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6\n",
      "[16:56:03] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 4/15] END alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6;, score=0.929 total time=  13.1s\n",
      "[CV 5/5; 4/15] START alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6\n",
      "[16:56:16] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 4/15] END alpha=0.4920847867923423, booster=gblinear, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.3, lambda=0.4172099580253376, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6;, score=0.934 total time=  13.0s\n",
      "[CV 1/5; 5/15] START alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0\n",
      "[16:56:29] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 5/15] END alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0;, score=0.928 total time= 1.6min\n",
      "[CV 2/5; 5/15] START alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0\n",
      "[16:58:08] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 5/15] END alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0;, score=0.912 total time= 1.8min\n",
      "[CV 3/5; 5/15] START alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0\n",
      "[16:59:54] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 5/15] END alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0;, score=0.939 total time= 1.7min\n",
      "[CV 4/5; 5/15] START alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0\n",
      "[17:01:37] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 5/15] END alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0;, score=0.925 total time= 1.7min\n",
      "[CV 5/5; 5/15] START alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0\n",
      "[17:03:20] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 5/15] END alpha=0.7659959601929949, booster=gblinear, colsample_bytree=0.6, eta=0.5, eval_metric=rmse, gamma=0.5, lambda=0.4936851076503062, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, subsample=1.0;, score=0.931 total time= 1.8min\n",
      "[CV 1/5; 6/15] START alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7\n",
      "[CV 1/5; 6/15] END alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7;, score=0.944 total time= 9.5min\n",
      "[CV 2/5; 6/15] START alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7\n",
      "[CV 2/5; 6/15] END alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7;, score=0.936 total time=10.1min\n",
      "[CV 3/5; 6/15] START alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7\n",
      "[CV 3/5; 6/15] END alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7;, score=0.950 total time= 9.6min\n",
      "[CV 4/5; 6/15] START alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7\n",
      "[CV 4/5; 6/15] END alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7;, score=0.926 total time= 9.5min\n",
      "[CV 5/5; 6/15] START alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7\n",
      "[CV 5/5; 6/15] END alpha=0.9441600282038797, booster=gbtree, colsample_bytree=0.9, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.3025494639535454, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, subsample=0.7;, score=0.955 total time=10.4min\n",
      "[CV 1/5; 7/15] START alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9\n",
      "[17:54:07] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 7/15] END alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9;, score=0.937 total time=  15.2s\n",
      "[CV 2/5; 7/15] START alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9\n",
      "[17:54:23] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 7/15] END alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9;, score=0.928 total time=  13.2s\n",
      "[CV 3/5; 7/15] START alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9\n",
      "[17:54:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 7/15] END alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9;, score=0.945 total time=  12.7s\n",
      "[CV 4/5; 7/15] START alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9\n",
      "[17:54:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 7/15] END alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9;, score=0.933 total time=  12.7s\n",
      "[CV 5/5; 7/15] START alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9\n",
      "[17:55:01] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 7/15] END alpha=0.48303427426270434, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.7515430037886347, max_depth=7, min_child_weight=4, n_estimators=50, objective=reg:tweedie, subsample=0.9;, score=0.934 total time=  12.8s\n",
      "[CV 1/5; 8/15] START alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7\n",
      "[17:55:14] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 8/15] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7;, score=0.934 total time=  12.7s\n",
      "[CV 2/5; 8/15] START alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7\n",
      "[17:55:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 8/15] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7;, score=0.922 total time=  13.0s\n",
      "[CV 3/5; 8/15] START alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7\n",
      "[17:55:40] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 8/15] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7;, score=0.942 total time=  12.9s\n",
      "[CV 4/5; 8/15] START alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7\n",
      "[17:55:53] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 8/15] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7;, score=0.928 total time=  13.0s\n",
      "[CV 5/5; 8/15] START alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7\n",
      "[17:56:06] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 8/15] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.689272016461775, max_depth=6, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7;, score=0.933 total time=  12.9s\n",
      "[CV 1/5; 9/15] START alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9\n",
      "[17:56:19] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 9/15] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9;, score=0.950 total time= 5.0min\n",
      "[CV 2/5; 9/15] START alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:01:20] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 9/15] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9;, score=0.923 total time= 5.1min\n",
      "[CV 3/5; 9/15] START alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9\n",
      "[18:06:23] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 9/15] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9;, score=0.934 total time= 5.0min\n",
      "[CV 4/5; 9/15] START alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9\n",
      "[18:11:25] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 9/15] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9;, score=0.933 total time= 5.0min\n",
      "[CV 5/5; 9/15] START alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9\n",
      "[18:16:25] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 9/15] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.8423424476202573, max_depth=6, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.9;, score=0.939 total time= 5.1min\n",
      "[CV 1/5; 10/15] START alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9\n",
      "[18:21:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 10/15] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9;, score=0.955 total time=   9.7s\n",
      "[CV 2/5; 10/15] START alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9\n",
      "[18:21:40] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 10/15] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9;, score=0.928 total time=   9.7s\n",
      "[CV 3/5; 10/15] START alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9\n",
      "[18:21:50] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 10/15] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9;, score=0.940 total time=   9.7s\n",
      "[CV 4/5; 10/15] START alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9\n",
      "[18:22:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 10/15] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9;, score=0.937 total time=   9.8s\n",
      "[CV 5/5; 10/15] START alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9\n",
      "[18:22:09] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 10/15] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.01612921669501683, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, subsample=0.9;, score=0.947 total time=   9.7s\n",
      "[CV 1/5; 11/15] START alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0\n",
      "[18:22:19] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 11/15] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0;, score=0.930 total time= 3.4min\n",
      "[CV 2/5; 11/15] START alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0\n",
      "[18:25:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 11/15] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0;, score=0.918 total time= 4.0min\n",
      "[CV 3/5; 11/15] START alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0\n",
      "[18:29:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 11/15] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0;, score=0.936 total time= 3.2min\n",
      "[CV 4/5; 11/15] START alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0\n",
      "[18:32:53] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 11/15] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0;, score=0.923 total time= 3.2min\n",
      "[CV 5/5; 11/15] START alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0\n",
      "[18:36:05] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 11/15] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, lambda=0.2183760235542644, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:tweedie, subsample=1.0;, score=0.928 total time= 3.3min\n",
      "[CV 1/5; 12/15] START alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6\n",
      "[18:39:21] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 12/15] END alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6;, score=0.343 total time= 1.4min\n",
      "[CV 2/5; 12/15] START alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6\n",
      "[18:40:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 2/5; 12/15] END alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6;, score=0.342 total time= 1.5min\n",
      "[CV 3/5; 12/15] START alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6\n",
      "[18:42:11] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 12/15] END alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6;, score=0.936 total time= 5.7min\n",
      "[CV 4/5; 12/15] START alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:47:51] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 12/15] END alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6;, score=0.931 total time= 5.7min\n",
      "[CV 5/5; 12/15] START alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6\n",
      "[18:53:32] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 12/15] END alpha=0.23130149131227545, booster=gblinear, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.3, lambda=0.00413465274939046, max_depth=7, min_child_weight=4, n_estimators=500, objective=reg:squarederror, subsample=0.6;, score=0.342 total time= 1.4min\n",
      "[CV 1/5; 13/15] START alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6\n",
      "[CV 1/5; 13/15] END alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6;, score=0.928 total time=  46.0s\n",
      "[CV 2/5; 13/15] START alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6\n",
      "[CV 2/5; 13/15] END alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6;, score=0.936 total time=  46.1s\n",
      "[CV 3/5; 13/15] START alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6\n",
      "[CV 3/5; 13/15] END alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6;, score=0.939 total time=  46.0s\n",
      "[CV 4/5; 13/15] START alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6\n",
      "[CV 4/5; 13/15] END alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6;, score=0.933 total time=  45.9s\n",
      "[CV 5/5; 13/15] START alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6\n",
      "[CV 5/5; 13/15] END alpha=0.15112746234808022, booster=gbtree, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.5, lambda=0.3434560240483249, max_depth=2, min_child_weight=4, n_estimators=50, objective=reg:squarederror, subsample=0.6;, score=0.951 total time=  46.1s\n",
      "[CV 1/5; 14/15] START alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7\n",
      "[CV 1/5; 14/15] END alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7;, score=0.931 total time= 2.7min\n",
      "[CV 2/5; 14/15] START alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7\n",
      "[CV 2/5; 14/15] END alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7;, score=0.933 total time= 2.5min\n",
      "[CV 3/5; 14/15] START alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7\n",
      "[CV 3/5; 14/15] END alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7;, score=0.942 total time= 2.6min\n",
      "[CV 4/5; 14/15] START alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7\n",
      "[CV 4/5; 14/15] END alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7;, score=0.933 total time= 2.6min\n",
      "[CV 5/5; 14/15] START alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7\n",
      "[CV 5/5; 14/15] END alpha=0.46023843440220963, booster=gbtree, colsample_bytree=1.0, eta=0.4, eval_metric=rmse, gamma=0.5, lambda=0.8179275184618664, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:tweedie, subsample=0.7;, score=0.951 total time= 2.6min\n",
      "[CV 1/5; 15/15] START alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7\n",
      "[19:11:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 1/5; 15/15] END alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.951 total time=  28.4s\n",
      "[CV 2/5; 15/15] START alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7\n",
      "[19:12:11] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 15/15] END alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.928 total time=  28.7s\n",
      "[CV 3/5; 15/15] START alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7\n",
      "[19:12:40] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 3/5; 15/15] END alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.936 total time=  28.5s\n",
      "[CV 4/5; 15/15] START alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7\n",
      "[19:13:08] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 4/5; 15/15] END alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.940 total time=  28.6s\n",
      "[CV 5/5; 15/15] START alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7\n",
      "[19:13:37] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV 5/5; 15/15] END alpha=0.17108183920509906, booster=gblinear, colsample_bytree=0.7, eta=0.4, eval_metric=rmse, gamma=0.3, lambda=0.578551478108833, max_depth=3, min_child_weight=4, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.939 total time=  28.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9442187783615662"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=5,\n",
    "    verbose=10,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search.fit(X2_train, y1_train)\n",
    "\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.6848297485848633,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bytree': 0.7,\n",
       " 'eta': 0.3,\n",
       " 'eval_metric': 'rmse',\n",
       " 'gamma': 0.5,\n",
       " 'lambda': 0.6273170193376167,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 5,\n",
       " 'n_estimators': 500,\n",
       " 'objective': 'reg:squarederror',\n",
       " 'subsample': 0.6}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  100.00%\n",
      "Test Accuracy:  95.18%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search.best_estimator_.score(X2_train, y1_train)*100: 0.2f}%\") \n",
    "print(f\"Test Accuracy: {search.best_estimator_.score(X2_test, y1_test)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv_acc = cross_val_score(estimator=XGBClassifier(random_state=123, \n",
    "                                                 use_label_encoder=False,\n",
    "                                                 alpha=0.6848297485848633,\n",
    "                                                 booster='gbtree',\n",
    "                                                 colsample_bytree=0.7,\n",
    "                                                 eta=0.3,\n",
    "                                                 eval_metric='rmse',\n",
    "                                                 gamma=0.5,\n",
    "                                                 'lambda'=0.6273170193376167,\n",
    "                                                 max_depth= 3,\n",
    "                                                 min_child_weight=5,\n",
    "                                                 n_estimators=500,\n",
    "                                                 objective='reg:squarederror',\n",
    "                                                 subsample=0.6),\n",
    "                         X=X2,\n",
    "                         y=y1,\n",
    "                         cv=StratifiedKFold(n_splits=10, random_state=123, shuffle=True),\n",
    "                         n_jobs=-1)\n",
    "\n",
    "print('Kfold Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.2s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.759635579937304"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=123)\n",
    "\n",
    "params = {\n",
    "    \"C\": scipy.stats.expon(scale=.01),\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"warm_start\": [True,False]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=lr,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=10,\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search.fit(X1_train, y1_train)\n",
    "\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.03950982068814718, 'fit_intercept': True, 'warm_start': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  76.68%\n",
      "Test Accuracy:  78.22%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search.best_estimator_.score(X1_train, y1_train)*100: 0.2f}%\") \n",
    "print(f\"Test Accuracy: {search.best_estimator_.score(X1_test, y1_test)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv_acc = cross_val_score(estimator=LogisticRegression(random_state=123, \n",
    "                                                      C=0.03950982068814718,\n",
    "                                                      fit_intercept=True,\n",
    "                                                      warm_start=False),\n",
    "                         X=X1,\n",
    "                         y=y1,\n",
    "                         cv=StratifiedKFold(n_splits=10, random_state=123, shuffle=True),\n",
    "                         n_jobs=-1)\n",
    "\n",
    "print('Kfold Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=  10.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=  10.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=  10.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=  10.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=  10.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=  10.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=  10.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=  10.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=  10.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=  10.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=  10.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=  10.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=  10.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=  10.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=  10.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=  10.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=  10.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=  11.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=  10.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=  10.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=  10.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=  10.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=  10.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=  10.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=  10.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=  10.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=  11.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=  11.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=  11.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=  11.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=  11.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=  11.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=  11.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=  11.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=  11.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=  11.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=  11.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=  11.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=  11.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=  11.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=  11.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=  11.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=  11.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=  11.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=  11.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=  11.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=  11.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=  11.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=  10.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=  11.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=  11.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=  11.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=  11.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=  11.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=  11.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=  11.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=  11.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.922281091635147"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = RandomizedSearchCV(\n",
    "    estimator=lr,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search.fit(X2_train, y1_train)\n",
    "\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.002572840801170508, 'fit_intercept': True, 'warm_start': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  100.00%\n",
      "Test Accuracy:  92.76%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search.best_estimator_.score(X2_train, y1_train)*100: 0.2f}%\") \n",
    "print(f\"Test Accuracy: {search.best_estimator_.score(X2_test, y1_test)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-251599ed4e33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m cv_acc = cross_val_score(estimator=LogisticRegression(random_state=123, \n\u001b[0m\u001b[1;32m      5\u001b[0m                                                       \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.002572840801170508\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                       \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv_acc = cross_val_score(estimator=LogisticRegression(random_state=123, \n",
    "                                                      C=0.002572840801170508,\n",
    "                                                      fit_intercept=True,\n",
    "                                                      warm_start=False),\n",
    "                         X=X2,\n",
    "                         y=y1,\n",
    "                         cv=StratifiedKFold(n_splits=10, random_state=123, shuffle=True),\n",
    "                         n_jobs=-1)\n",
    "\n",
    "print('Kfold Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree + Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8691232109282069"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "params =  {\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'max_depth': [5, 7, 9, 11, None],\n",
    "    'criterion':['entropy', 'gini']\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=tree,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search.fit(X1_train_sub, y1_train_sub)\n",
    "\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 3, 'max_depth': 5, 'criterion': 'gini'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  91.77%\n",
      "Valid Accuracy:  87.32%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search.best_estimator_.score(X1_train_sub, y1_train_sub)*100: 0.2f}%\") \n",
    "print(f\"Valid Accuracy: {search.best_estimator_.score(X1_valid, y1_valid)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 93.064%\n",
      "Valid Accuracy: 88.576%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag = BaggingClassifier(base_estimator=search.best_estimator_, \n",
    "                        n_estimators=150, \n",
    "                        oob_score=True, \n",
    "                        bootstrap=True,\n",
    "                        bootstrap_features=False, \n",
    "                        n_jobs=1, \n",
    "                        random_state=123)\n",
    "\n",
    "bag.fit(X1_train_sub, y1_train_sub)\n",
    "print(f\"Train Accuracy: {bag.score(X1_train_sub, y1_train_sub)*100:0.3f}%\")\n",
    "print(f\"Valid Accuracy: {bag.score(X1_valid, y1_valid)*100:0.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv_acc = cross_val_score(estimator=bag,\n",
    "                         X=X1,\n",
    "                         y=y1,\n",
    "                         cv=StratifiedKFold(n_splits=10, random_state=123, shuffle=True),\n",
    "                         n_jobs=-1)\n",
    "\n",
    "print('Kfold Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_estimator_ = DecisionTreeClassifier(max_depth=18, \n",
    "                                                min_impurity_decrease=0.017492913234125385,\n",
    "                                                min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 91.066%\n",
      "Valid Accuracy: 89.984%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag = BaggingClassifier(base_estimator=search.best_estimator_, \n",
    "                        n_estimators=50, \n",
    "                        oob_score=True, \n",
    "                        bootstrap=True,\n",
    "                        bootstrap_features=False, \n",
    "                        n_jobs=1, \n",
    "                        random_state=123)\n",
    "\n",
    "bag.fit(X2_train_sub, y1_train_sub)\n",
    "print(f\"Train Accuracy: {bag.score(X2_train_sub, y1_train_sub)*100:0.3f}%\")\n",
    "print(f\"Valid Accuracy: {bag.score(X2_valid, y1_valid)*100:0.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv_acc = cross_val_score(estimator=bag,\n",
    "                         X=X2,\n",
    "                         y=y1,\n",
    "                         cv=StratifiedKFold(n_splits=10, random_state=123, shuffle=True),\n",
    "                         n_jobs=-1)\n",
    "\n",
    "print('Kfold Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
