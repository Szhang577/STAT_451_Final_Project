{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape: (4559, 512)\n",
      "X2.shape: (4559, 30000)\n",
      "y1.shape: (4559,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X1 = pd.read_csv('../data/features.csv', header=None).values\n",
    "X2 = pd.read_csv('../data/raw_images.csv', header=None).values\n",
    "y1 = pd.read_csv('../data/labels.csv', header=None).values.ravel().astype(int)\n",
    "\n",
    "print('X1.shape:', X1.shape)\n",
    "print('X2.shape:', X2.shape)\n",
    "print('y1.shape:', y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test sizes: 3191 639 1368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = \\\n",
    "    train_test_split(X1, y1, test_size=0.3, random_state=123, shuffle=True, stratify=y1)\n",
    "\n",
    "X1_train_sub, X1_valid, y1_train_sub, y1_valid = \\\n",
    "    train_test_split(X1_train, y1_train, test_size=0.2, random_state=123, stratify=y1_train)\n",
    "\n",
    "print('Train/Valid/Test sizes:', y1_train.shape[0], y1_valid.shape[0], y1_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test sizes: 3191 639 1368\n"
     ]
    }
   ],
   "source": [
    "X2_train, X2_test, y1_train, y1_test = \\\n",
    "    train_test_split(X2, y1, test_size=0.3, random_state=123, shuffle=True, stratify=y1)\n",
    "\n",
    "X2_train_sub, X2_valid, y1_train_sub, y1_valid = \\\n",
    "    train_test_split(X2_train, y1_train, test_size=0.2, random_state=123, stratify=y1_train)\n",
    "\n",
    "print('Train/Valid/Test sizes:', y1_train.shape[0], y1_valid.shape[0], y1_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7; total time=   2.8s\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7; total time=   2.7s\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7; total time=   2.8s\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7; total time=   3.1s\n",
      "[CV] END alpha=0.6964691955978617, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:tweedie, reg_lambda=0.7800277719120792, subsample=0.7; total time=   3.0s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0; total time=   1.2s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0; total time=   1.3s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0; total time=   1.3s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0; total time=   1.2s\n",
      "[CV] END alpha=0.6848297485848633, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=5, n_estimators=50, objective=reg:tweedie, reg_lambda=0.24475928695391985, subsample=1.0; total time=   1.2s\n",
      "[CV] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7; total time=   1.2s\n",
      "[CV] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7; total time=   1.2s\n",
      "[CV] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7; total time=   1.1s\n",
      "[CV] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7; total time=   1.2s\n",
      "[CV] END alpha=0.3980442653304314, booster=gbtree, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=4, min_child_weight=4, n_estimators=30, objective=reg:tweedie, reg_lambda=0.8494318040777896, subsample=0.7; total time=   1.4s\n",
      "[CV] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8; total time=   3.4s\n",
      "[CV] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8; total time=   3.1s\n",
      "[CV] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8; total time=   3.8s\n",
      "[CV] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8; total time=   3.6s\n",
      "[CV] END alpha=0.7402964007347881, booster=gbtree, colsample_bytree=1.0, eta=0.5, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=4, n_estimators=50, objective=reg:squarederror, reg_lambda=0.6309761338544878, subsample=0.8; total time=   3.7s\n",
      "[22:49:59] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0; total time=   1.4s\n",
      "[22:50:01] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0; total time=   1.4s\n",
      "[22:50:02] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0; total time=   1.3s\n",
      "[22:50:03] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0; total time=   1.3s\n",
      "[22:50:05] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.7770044488897805, booster=gblinear, colsample_bytree=0.8, eta=0.5, eval_metric=rmse, gamma=0.5, max_depth=4, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.8933891731171348, subsample=1.0; total time=   1.3s\n",
      "[22:50:06] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9; total time=   1.3s\n",
      "[22:50:07] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9; total time=   1.4s\n",
      "[22:50:09] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9; total time=   1.4s\n",
      "[22:50:10] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9; total time=   1.4s\n",
      "[22:50:12] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.5018366858843366, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=300, objective=reg:squarederror, reg_lambda=0.2504553753965067, subsample=0.9; total time=   1.3s\n",
      "[22:50:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9; total time=   2.2s\n",
      "[22:50:15] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9; total time=   2.3s\n",
      "[22:50:17] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9; total time=   2.3s\n",
      "[22:50:20] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9; total time=   2.2s\n",
      "[22:50:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.4954917976836641, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=2, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.797728418895881, subsample=0.9; total time=   2.3s\n",
      "[22:50:24] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7; total time=   1.4s\n",
      "[22:50:26] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7; total time=   1.4s\n",
      "[22:50:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7; total time=   1.4s\n",
      "[22:50:28] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7; total time=   1.3s\n",
      "[22:50:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.545068016466465, booster=gblinear, colsample_bytree=0.6, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=6, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5104223474780112, subsample=0.7; total time=   1.4s\n",
      "[22:50:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9; total time=   1.4s\n",
      "[22:50:32] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9; total time=   1.4s\n",
      "[22:50:34] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9; total time=   1.4s\n",
      "[22:50:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9; total time=   1.4s\n",
      "[22:50:37] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.26828131771172453, booster=gblinear, colsample_bytree=0.9, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=300, objective=reg:tweedie, reg_lambda=0.18033629310429272, subsample=0.9; total time=   1.4s\n",
      "[22:50:38] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9; total time=   0.2s\n",
      "[22:50:38] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9; total time=   0.2s\n",
      "[22:50:38] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9; total time=   0.1s\n",
      "[22:50:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9; total time=   0.2s\n",
      "[22:50:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.21229811338998222, booster=gblinear, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=2, min_child_weight=5, n_estimators=30, objective=reg:tweedie, reg_lambda=0.15895965414472274, subsample=0.9; total time=   0.2s\n",
      "[22:50:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0; total time=   2.3s\n",
      "[22:50:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0; total time=   2.3s\n",
      "[22:50:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0; total time=   2.2s\n",
      "[22:50:46] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0; total time=   2.3s\n",
      "[22:50:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.695529538770911, booster=gblinear, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.4, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_lambda=0.8549728258240765, subsample=1.0; total time=   2.3s\n",
      "[CV] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0; total time=  17.9s\n",
      "[CV] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0; total time=  18.1s\n",
      "[CV] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0; total time=  16.9s\n",
      "[CV] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0; total time=  19.0s\n",
      "[CV] END alpha=0.3929444207680876, booster=gbtree, colsample_bytree=1.0, eta=0.3, eval_metric=rmse, gamma=0.5, max_depth=2, min_child_weight=4, n_estimators=500, objective=reg:squarederror, reg_lambda=0.0576480249714419, subsample=1.0; total time=  17.1s\n",
      "[CV] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7; total time=   5.0s\n",
      "[CV] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7; total time=   5.3s\n",
      "[CV] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7; total time=   4.7s\n",
      "[CV] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7; total time=   4.6s\n",
      "[CV] END alpha=0.6917018087001772, booster=gbtree, colsample_bytree=0.7, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=7, min_child_weight=4, n_estimators=100, objective=reg:tweedie, reg_lambda=0.029423743551983135, subsample=0.7; total time=   5.1s\n",
      "[CV] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0; total time=  11.7s\n",
      "[CV] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0; total time=  12.4s\n",
      "[CV] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0; total time=  12.2s\n",
      "[CV] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0; total time=  11.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.13089496066408074, booster=gbtree, colsample_bytree=0.8, eta=0.3, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=4, n_estimators=300, objective=reg:tweedie, reg_lambda=0.5594873855040602, subsample=1.0; total time=  11.6s\n",
      "[22:53:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7; total time=   0.3s\n",
      "[22:53:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7; total time=   0.3s\n",
      "[22:53:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7; total time=   0.2s\n",
      "[22:53:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7; total time=   0.2s\n",
      "[22:53:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[CV] END alpha=0.31678790711837973, booster=gblinear, colsample_bytree=0.7, eta=0.5, eval_metric=rmse, gamma=0.3, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, reg_lambda=0.578551478108833, subsample=0.7; total time=   0.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9197712923307872"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(random_state=123, use_label_encoder=False)\n",
    "\n",
    "params = {\n",
    "    'n_estimators':[30, 50, 100, 300, 500],\n",
    "    'min_child_weight':[4,5], \n",
    "    \"reg_lambda\": scipy.stats.uniform(1e-8, 1.0),\n",
    "    \"alpha\": scipy.stats.uniform(1e-8, 1.0),\n",
    "    'gamma':[i/10.0 for i in range(3,6)],  \n",
    "    'subsample':[i/10.0 for i in range(6,11)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,11)], \n",
    "    'max_depth': [2,3,4,6,7],\n",
    "    'objective': ['reg:squarederror', 'reg:tweedie'],\n",
    "    'booster': ['gbtree', 'gblinear'],\n",
    "    'eval_metric': ['rmse'],\n",
    "    'eta': [i/10.0 for i in range(3,6)],\n",
    "}\n",
    "\n",
    "search1 = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search1.fit(X1_train, y1_train)\n",
    "\n",
    "search1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.6917018087001772,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bytree': 0.7,\n",
       " 'eta': 0.3,\n",
       " 'eval_metric': 'rmse',\n",
       " 'gamma': 0.3,\n",
       " 'max_depth': 7,\n",
       " 'min_child_weight': 4,\n",
       " 'n_estimators': 100,\n",
       " 'objective': 'reg:tweedie',\n",
       " 'reg_lambda': 0.029423743551983135,\n",
       " 'subsample': 0.7}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  99.94%\n",
      "Test Accuracy:  91.74%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search1.best_estimator_.score(X1_train, y1_train)*100: 0.2f}%\") \n",
    "print(f\"Test Accuracy: {search1.best_estimator_.score(X1_test, y1_test)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9170036919863896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 score: {f1_score(y1_test, search1.best_estimator_.predict(X1_test), average='weighted')}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV 1/5; 1/15] START criterion=entropy, max_depth=9, min_samples_split=3........\n",
      "[02:10:51] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:11:01] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 1/15] END criterion=entropy, max_depth=9, min_samples_split=3;, score=0.939 total time= 5.4min\n",
      "[CV 2/5; 1/15] START criterion=entropy, max_depth=9, min_samples_split=3........\n",
      "[02:16:17] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:16:26] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 1/15] END criterion=entropy, max_depth=9, min_samples_split=3;, score=0.931 total time= 5.3min\n",
      "[CV 3/5; 1/15] START criterion=entropy, max_depth=9, min_samples_split=3........\n",
      "[02:21:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:21:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 1/15] END criterion=entropy, max_depth=9, min_samples_split=3;, score=0.951 total time= 5.3min\n",
      "[CV 4/5; 1/15] START criterion=entropy, max_depth=9, min_samples_split=3........\n",
      "[02:26:50] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:26:59] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 1/15] END criterion=entropy, max_depth=9, min_samples_split=3;, score=0.929 total time= 5.5min\n",
      "[CV 5/5; 1/15] START criterion=entropy, max_depth=9, min_samples_split=3........\n",
      "[02:32:18] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:32:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 1/15] END criterion=entropy, max_depth=9, min_samples_split=3;, score=0.953 total time= 5.5min\n",
      "[CV 1/5; 2/15] START criterion=gini, max_depth=None, min_samples_split=4........\n",
      "[02:37:47] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:37:54] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 2/15] END criterion=gini, max_depth=None, min_samples_split=4;, score=0.936 total time= 4.9min\n",
      "[CV 2/5; 2/15] START criterion=gini, max_depth=None, min_samples_split=4........\n",
      "[02:42:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:42:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 2/15] END criterion=gini, max_depth=None, min_samples_split=4;, score=0.940 total time= 4.9min\n",
      "[CV 3/5; 2/15] START criterion=gini, max_depth=None, min_samples_split=4........\n",
      "[02:47:33] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:47:40] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 2/15] END criterion=gini, max_depth=None, min_samples_split=4;, score=0.956 total time= 4.9min\n",
      "[CV 4/5; 2/15] START criterion=gini, max_depth=None, min_samples_split=4........\n",
      "[02:52:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:52:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 2/15] END criterion=gini, max_depth=None, min_samples_split=4;, score=0.937 total time= 4.9min\n",
      "[CV 5/5; 2/15] START criterion=gini, max_depth=None, min_samples_split=4........\n",
      "[02:57:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:57:29] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 2/15] END criterion=gini, max_depth=None, min_samples_split=4;, score=0.950 total time= 5.1min\n",
      "[CV 1/5; 3/15] START criterion=entropy, max_depth=7, min_samples_split=4........\n",
      "[03:02:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:02:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 3/15] END criterion=entropy, max_depth=7, min_samples_split=4;, score=0.941 total time= 5.1min\n",
      "[CV 2/5; 3/15] START criterion=entropy, max_depth=7, min_samples_split=4........\n",
      "[03:07:34] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:07:42] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 3/15] END criterion=entropy, max_depth=7, min_samples_split=4;, score=0.934 total time= 5.0min\n",
      "[CV 3/5; 3/15] START criterion=entropy, max_depth=7, min_samples_split=4........\n",
      "[03:12:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:12:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 3/15] END criterion=entropy, max_depth=7, min_samples_split=4;, score=0.953 total time= 5.1min\n",
      "[CV 4/5; 3/15] START criterion=entropy, max_depth=7, min_samples_split=4........\n",
      "[03:17:42] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:17:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 3/15] END criterion=entropy, max_depth=7, min_samples_split=4;, score=0.928 total time= 5.1min\n",
      "[CV 5/5; 3/15] START criterion=entropy, max_depth=7, min_samples_split=4........\n",
      "[03:22:47] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:22:54] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 3/15] END criterion=entropy, max_depth=7, min_samples_split=4;, score=0.951 total time= 5.2min\n",
      "[CV 1/5; 4/15] START criterion=gini, max_depth=11, min_samples_split=4..........\n",
      "[03:28:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:28:11] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 4/15] END criterion=gini, max_depth=11, min_samples_split=4;, score=0.937 total time= 5.5min\n",
      "[CV 2/5; 4/15] START criterion=gini, max_depth=11, min_samples_split=4..........\n",
      "[03:33:29] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:33:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 4/15] END criterion=gini, max_depth=11, min_samples_split=4;, score=0.934 total time= 5.3min\n",
      "[CV 3/5; 4/15] START criterion=gini, max_depth=11, min_samples_split=4..........\n",
      "[03:38:47] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:38:57] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 4/15] END criterion=gini, max_depth=11, min_samples_split=4;, score=0.951 total time= 5.4min\n",
      "[CV 4/5; 4/15] START criterion=gini, max_depth=11, min_samples_split=4..........\n",
      "[03:44:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:44:23] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 4/15] END criterion=gini, max_depth=11, min_samples_split=4;, score=0.931 total time= 5.4min\n",
      "[CV 5/5; 4/15] START criterion=gini, max_depth=11, min_samples_split=4..........\n",
      "[03:49:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:49:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 4/15] END criterion=gini, max_depth=11, min_samples_split=4;, score=0.951 total time= 5.5min\n",
      "[CV 1/5; 5/15] START criterion=entropy, max_depth=9, min_samples_split=4........\n",
      "[03:55:12] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:55:21] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 5/15] END criterion=entropy, max_depth=9, min_samples_split=4;, score=0.939 total time= 5.3min\n",
      "[CV 2/5; 5/15] START criterion=entropy, max_depth=9, min_samples_split=4........\n",
      "[04:00:28] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:00:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 5/15] END criterion=entropy, max_depth=9, min_samples_split=4;, score=0.931 total time= 5.2min\n",
      "[CV 3/5; 5/15] START criterion=entropy, max_depth=9, min_samples_split=4........\n",
      "[04:05:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:05:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 5/15] END criterion=entropy, max_depth=9, min_samples_split=4;, score=0.951 total time= 5.4min\n",
      "[CV 4/5; 5/15] START criterion=entropy, max_depth=9, min_samples_split=4........\n",
      "[04:11:03] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:11:12] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 5/15] END criterion=entropy, max_depth=9, min_samples_split=4;, score=0.929 total time= 5.2min\n",
      "[CV 5/5; 5/15] START criterion=entropy, max_depth=9, min_samples_split=4........\n",
      "[04:16:15] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:16:24] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 5/15] END criterion=entropy, max_depth=9, min_samples_split=4;, score=0.953 total time= 5.4min\n",
      "[CV 1/5; 6/15] START criterion=gini, max_depth=None, min_samples_split=2........\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:21:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:21:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 6/15] END criterion=gini, max_depth=None, min_samples_split=2;, score=0.936 total time= 4.9min\n",
      "[CV 2/5; 6/15] START criterion=gini, max_depth=None, min_samples_split=2........\n",
      "[04:26:37] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:26:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 6/15] END criterion=gini, max_depth=None, min_samples_split=2;, score=0.940 total time= 4.8min\n",
      "[CV 3/5; 6/15] START criterion=gini, max_depth=None, min_samples_split=2........\n",
      "[04:31:28] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:31:34] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 6/15] END criterion=gini, max_depth=None, min_samples_split=2;, score=0.956 total time= 4.9min\n",
      "[CV 4/5; 6/15] START criterion=gini, max_depth=None, min_samples_split=2........\n",
      "[04:36:23] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:36:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 6/15] END criterion=gini, max_depth=None, min_samples_split=2;, score=0.937 total time= 4.8min\n",
      "[CV 5/5; 6/15] START criterion=gini, max_depth=None, min_samples_split=2........\n",
      "[04:41:12] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:41:19] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 6/15] END criterion=gini, max_depth=None, min_samples_split=2;, score=0.950 total time= 5.0min\n",
      "[CV 1/5; 7/15] START criterion=entropy, max_depth=None, min_samples_split=2.....\n",
      "[04:46:12] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:46:19] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 7/15] END criterion=entropy, max_depth=None, min_samples_split=2;, score=0.936 total time= 4.9min\n",
      "[CV 2/5; 7/15] START criterion=entropy, max_depth=None, min_samples_split=2.....\n",
      "[04:51:06] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:51:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 7/15] END criterion=entropy, max_depth=None, min_samples_split=2;, score=0.940 total time= 4.8min\n",
      "[CV 3/5; 7/15] START criterion=entropy, max_depth=None, min_samples_split=2.....\n",
      "[04:55:53] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:56:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 7/15] END criterion=entropy, max_depth=None, min_samples_split=2;, score=0.956 total time= 4.9min\n",
      "[CV 4/5; 7/15] START criterion=entropy, max_depth=None, min_samples_split=2.....\n",
      "[05:00:46] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:00:52] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 7/15] END criterion=entropy, max_depth=None, min_samples_split=2;, score=0.937 total time= 4.8min\n",
      "[CV 5/5; 7/15] START criterion=entropy, max_depth=None, min_samples_split=2.....\n",
      "[05:05:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[05:05:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 7/15] END criterion=entropy, max_depth=None, min_samples_split=2;, score=0.950 total time= 5.0min\n",
      "[CV 1/5; 8/15] START criterion=gini, max_depth=9, min_samples_split=2...........\n",
      "[05:10:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[05:10:45] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 8/15] END criterion=gini, max_depth=9, min_samples_split=2;, score=0.939 total time= 5.3min\n",
      "[CV 2/5; 8/15] START criterion=gini, max_depth=9, min_samples_split=2...........\n",
      "[05:15:52] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[05:16:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 8/15] END criterion=gini, max_depth=9, min_samples_split=2;, score=0.931 total time= 5.2min\n",
      "[CV 3/5; 8/15] START criterion=gini, max_depth=9, min_samples_split=2...........\n",
      "[05:21:03] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[05:21:12] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 8/15] END criterion=gini, max_depth=9, min_samples_split=2;, score=0.951 total time= 5.2min\n",
      "[CV 4/5; 8/15] START criterion=gini, max_depth=9, min_samples_split=2...........\n",
      "[05:26:18] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[05:26:26] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 8/15] END criterion=gini, max_depth=9, min_samples_split=2;, score=0.929 total time= 5.2min\n",
      "[CV 5/5; 8/15] START criterion=gini, max_depth=9, min_samples_split=2...........\n",
      "[05:31:29] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[05:31:38] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 8/15] END criterion=gini, max_depth=9, min_samples_split=2;, score=0.953 total time= 5.3min\n",
      "[CV 1/5; 9/15] START criterion=entropy, max_depth=11, min_samples_split=4.......\n",
      "[05:36:50] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[05:37:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 9/15] END criterion=entropy, max_depth=11, min_samples_split=4;, score=0.937 total time= 5.4min\n",
      "[CV 2/5; 9/15] START criterion=entropy, max_depth=11, min_samples_split=4.......\n",
      "[05:42:16] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[05:42:26] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 9/15] END criterion=entropy, max_depth=11, min_samples_split=4;, score=0.934 total time= 5.3min\n",
      "[CV 3/5; 9/15] START criterion=entropy, max_depth=11, min_samples_split=4.......\n",
      "[05:47:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[05:47:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 9/15] END criterion=entropy, max_depth=11, min_samples_split=4;, score=0.951 total time= 5.4min\n",
      "[CV 4/5; 9/15] START criterion=entropy, max_depth=11, min_samples_split=4.......\n",
      "[05:52:56] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[05:53:06] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 9/15] END criterion=entropy, max_depth=11, min_samples_split=4;, score=0.931 total time= 5.3min\n",
      "[CV 5/5; 9/15] START criterion=entropy, max_depth=11, min_samples_split=4.......\n",
      "[05:58:16] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[05:58:26] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 9/15] END criterion=entropy, max_depth=11, min_samples_split=4;, score=0.951 total time= 5.5min\n",
      "[CV 1/5; 10/15] START criterion=gini, max_depth=9, min_samples_split=4..........\n",
      "[06:03:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[06:03:57] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 10/15] END criterion=gini, max_depth=9, min_samples_split=4;, score=0.939 total time= 5.3min\n",
      "[CV 2/5; 10/15] START criterion=gini, max_depth=9, min_samples_split=4..........\n",
      "[06:09:03] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[06:09:12] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 10/15] END criterion=gini, max_depth=9, min_samples_split=4;, score=0.931 total time= 5.2min\n",
      "[CV 3/5; 10/15] START criterion=gini, max_depth=9, min_samples_split=4..........\n",
      "[06:14:14] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[06:14:23] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 10/15] END criterion=gini, max_depth=9, min_samples_split=4;, score=0.951 total time= 5.2min\n",
      "[CV 4/5; 10/15] START criterion=gini, max_depth=9, min_samples_split=4..........\n",
      "[06:19:29] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[06:19:38] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 10/15] END criterion=gini, max_depth=9, min_samples_split=4;, score=0.929 total time= 5.2min\n",
      "[CV 5/5; 10/15] START criterion=gini, max_depth=9, min_samples_split=4..........\n",
      "[06:24:40] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[06:24:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 10/15] END criterion=gini, max_depth=9, min_samples_split=4;, score=0.953 total time= 5.4min\n",
      "[CV 1/5; 11/15] START criterion=gini, max_depth=7, min_samples_split=2..........\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:30:03] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[06:30:10] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 11/15] END criterion=gini, max_depth=7, min_samples_split=2;, score=0.941 total time= 5.0min\n",
      "[CV 2/5; 11/15] START criterion=gini, max_depth=7, min_samples_split=2..........\n",
      "[06:35:05] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[06:35:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 11/15] END criterion=gini, max_depth=7, min_samples_split=2;, score=0.934 total time= 5.0min\n",
      "[CV 3/5; 11/15] START criterion=gini, max_depth=7, min_samples_split=2..........\n",
      "[06:40:03] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[06:40:11] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 11/15] END criterion=gini, max_depth=7, min_samples_split=2;, score=0.953 total time= 5.0min\n",
      "[CV 4/5; 11/15] START criterion=gini, max_depth=7, min_samples_split=2..........\n",
      "[06:45:04] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[06:45:11] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 11/15] END criterion=gini, max_depth=7, min_samples_split=2;, score=0.928 total time= 5.0min\n",
      "[CV 5/5; 11/15] START criterion=gini, max_depth=7, min_samples_split=2..........\n",
      "[06:50:03] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[06:50:10] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 11/15] END criterion=gini, max_depth=7, min_samples_split=2;, score=0.951 total time= 5.1min\n",
      "[CV 1/5; 12/15] START criterion=entropy, max_depth=7, min_samples_split=3.......\n",
      "[06:55:11] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[06:55:19] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 12/15] END criterion=entropy, max_depth=7, min_samples_split=3;, score=0.941 total time= 5.0min\n",
      "[CV 2/5; 12/15] START criterion=entropy, max_depth=7, min_samples_split=3.......\n",
      "[07:00:14] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:00:21] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 12/15] END criterion=entropy, max_depth=7, min_samples_split=3;, score=0.934 total time= 4.9min\n",
      "[CV 3/5; 12/15] START criterion=entropy, max_depth=7, min_samples_split=3.......\n",
      "[07:05:10] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:05:18] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 12/15] END criterion=entropy, max_depth=7, min_samples_split=3;, score=0.953 total time= 5.0min\n",
      "[CV 4/5; 12/15] START criterion=entropy, max_depth=7, min_samples_split=3.......\n",
      "[07:10:12] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:10:19] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 12/15] END criterion=entropy, max_depth=7, min_samples_split=3;, score=0.928 total time= 5.0min\n",
      "[CV 5/5; 12/15] START criterion=entropy, max_depth=7, min_samples_split=3.......\n",
      "[07:15:10] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:15:17] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 12/15] END criterion=entropy, max_depth=7, min_samples_split=3;, score=0.951 total time= 5.2min\n",
      "[CV 1/5; 13/15] START criterion=entropy, max_depth=7, min_samples_split=2.......\n",
      "[07:20:19] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:20:26] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 13/15] END criterion=entropy, max_depth=7, min_samples_split=2;, score=0.941 total time= 5.1min\n",
      "[CV 2/5; 13/15] START criterion=entropy, max_depth=7, min_samples_split=2.......\n",
      "[07:25:23] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:25:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 13/15] END criterion=entropy, max_depth=7, min_samples_split=2;, score=0.934 total time= 4.9min\n",
      "[CV 3/5; 13/15] START criterion=entropy, max_depth=7, min_samples_split=2.......\n",
      "[07:30:20] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:30:28] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 13/15] END criterion=entropy, max_depth=7, min_samples_split=2;, score=0.953 total time= 5.0min\n",
      "[CV 4/5; 13/15] START criterion=entropy, max_depth=7, min_samples_split=2.......\n",
      "[07:35:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:35:29] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 13/15] END criterion=entropy, max_depth=7, min_samples_split=2;, score=0.928 total time= 5.0min\n",
      "[CV 5/5; 13/15] START criterion=entropy, max_depth=7, min_samples_split=2.......\n",
      "[07:40:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:40:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 13/15] END criterion=entropy, max_depth=7, min_samples_split=2;, score=0.951 total time= 5.1min\n",
      "[CV 1/5; 14/15] START criterion=gini, max_depth=11, min_samples_split=2.........\n",
      "[07:45:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:45:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 14/15] END criterion=gini, max_depth=11, min_samples_split=2;, score=0.937 total time= 5.4min\n",
      "[CV 2/5; 14/15] START criterion=gini, max_depth=11, min_samples_split=2.........\n",
      "[07:50:58] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:51:07] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 14/15] END criterion=gini, max_depth=11, min_samples_split=2;, score=0.934 total time= 5.3min\n",
      "[CV 3/5; 14/15] START criterion=gini, max_depth=11, min_samples_split=2.........\n",
      "[07:56:15] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:56:24] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 14/15] END criterion=gini, max_depth=11, min_samples_split=2;, score=0.951 total time= 5.4min\n",
      "[CV 4/5; 14/15] START criterion=gini, max_depth=11, min_samples_split=2.........\n",
      "[08:01:37] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:01:47] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 14/15] END criterion=gini, max_depth=11, min_samples_split=2;, score=0.931 total time= 5.3min\n",
      "[CV 5/5; 14/15] START criterion=gini, max_depth=11, min_samples_split=2.........\n",
      "[08:06:57] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:07:07] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 14/15] END criterion=gini, max_depth=11, min_samples_split=2;, score=0.951 total time= 5.5min\n",
      "[CV 1/5; 15/15] START criterion=gini, max_depth=5, min_samples_split=3..........\n",
      "[08:12:29] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:12:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 15/15] END criterion=gini, max_depth=5, min_samples_split=3;, score=0.937 total time= 4.7min\n",
      "[CV 2/5; 15/15] START criterion=gini, max_depth=5, min_samples_split=3..........\n",
      "[08:17:10] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:17:15] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 15/15] END criterion=gini, max_depth=5, min_samples_split=3;, score=0.933 total time= 4.6min\n",
      "[CV 3/5; 15/15] START criterion=gini, max_depth=5, min_samples_split=3..........\n",
      "[08:21:47] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:21:52] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 15/15] END criterion=gini, max_depth=5, min_samples_split=3;, score=0.958 total time= 4.7min\n",
      "[CV 4/5; 15/15] START criterion=gini, max_depth=5, min_samples_split=3..........\n",
      "[08:26:26] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:26:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 15/15] END criterion=gini, max_depth=5, min_samples_split=3;, score=0.940 total time= 4.6min\n",
      "[CV 5/5; 15/15] START criterion=gini, max_depth=5, min_samples_split=3..........\n",
      "[08:31:04] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:31:10] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 15/15] END criterion=gini, max_depth=5, min_samples_split=3;, score=0.950 total time= 4.8min\n",
      "[08:35:50] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { criterion, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:35:58] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9439072610515058"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search2 = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=5,\n",
    "    verbose=10,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search2.fit(X2_train, y1_train)\n",
    "\n",
    "search2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 4, 'max_depth': None, 'criterion': 'gini'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  100.00%\n",
      "Test Accuracy:  94.81%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search2.best_estimator_.score(X2_train, y1_train)*100: 0.2f}%\") \n",
    "print(f\"Test Accuracy: {search2.best_estimator_.score(X2_test, y1_test)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9480100060014328\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 score: {f1_score(y1_test, search2.best_estimator_.predict(X2_test), average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.1s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.759635579937304"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=123, max_iter = 1000)\n",
    "\n",
    "params = {\n",
    "    \"C\": scipy.stats.expon(scale=.01),\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"warm_start\": [True,False]\n",
    "}\n",
    "\n",
    "search3 = RandomizedSearchCV(\n",
    "    estimator=lr,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=10,\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search3.fit(X1_train, y1_train)\n",
    "\n",
    "search3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.03950982068814718, 'fit_intercept': True, 'warm_start': False}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  76.68%\n",
      "Test Accuracy:  78.22%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search3.best_estimator_.score(X1_train, y1_train)*100: 0.2f}%\") \n",
    "print(f\"Test Accuracy: {search3.best_estimator_.score(X1_test, y1_test)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7691277689909498\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 score: {f1_score(y1_test, search3.best_estimator_.predict(X1_test), average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time= 1.7min\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time= 1.9min\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time= 1.7min\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time= 1.5min\n",
      "[CV] END C=0.011922721434811058, fit_intercept=True, warm_start=True; total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time= 1.8min\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time= 1.7min\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time= 1.7min\n",
      "[CV] END C=0.002572840801170508, fit_intercept=True, warm_start=False; total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time= 1.7min\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.012710709354874424, fit_intercept=False, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time= 1.4min\n",
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.03950982068814718, fit_intercept=True, warm_start=False; total time= 1.7min\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time= 1.7min\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time= 1.3min\n",
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.006557201934108684, fit_intercept=False, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004203422502573983, fit_intercept=False, warm_start=False; total time= 1.7min\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time= 1.5min\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time= 1.5min\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time= 1.5min\n",
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.005772721768030297, fit_intercept=False, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.005075713502135167, fit_intercept=True, warm_start=False; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0020149426060868045, fit_intercept=True, warm_start=True; total time= 1.7min\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time= 1.7min\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.007583288393287821, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time= 1.4min\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.010062180612636784, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time= 1.5min\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.012890055033202343, fit_intercept=True, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.012817303398323033, fit_intercept=False, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time= 1.5min\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.004490857897120136, fit_intercept=False, warm_start=True; total time= 1.6min\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time= 1.5min\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time= 1.6min\n",
      "[CV] END C=0.0034773509004921363, fit_intercept=True, warm_start=False; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9288631825785784"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search4 = RandomizedSearchCV(\n",
    "    estimator=lr,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search4.fit(X2_train, y1_train)\n",
    "\n",
    "search4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.002572840801170508, 'fit_intercept': True, 'warm_start': False}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  100.00%\n",
      "Test Accuracy:  93.20%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search4.best_estimator_.score(X2_train, y1_train)*100: 0.2f}%\") \n",
    "print(f\"Test Accuracy: {search4.best_estimator_.score(X2_test, y1_test)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.931976655005432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 score: {f1_score(y1_test, search4.best_estimator_.predict(X2_test), average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree + Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=4; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=9, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=2; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=11, min_samples_split=4; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=9, min_samples_split=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=3; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.3s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=7, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.3s\n",
      "[CV] END ..criterion=gini, max_depth=11, min_samples_split=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=5, min_samples_split=3; total time=   0.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8691232109282069"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "params =  {\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'max_depth': [5, 7, 9, 11, None],\n",
    "    'criterion':['entropy', 'gini']\n",
    "}\n",
    "\n",
    "search5 = RandomizedSearchCV(\n",
    "    estimator=tree,\n",
    "    param_distributions=params,\n",
    "    n_iter=15,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    "    random_state=123)\n",
    "\n",
    "search5.fit(X1_train_sub, y1_train_sub)\n",
    "\n",
    "search5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 3, 'max_depth': 5, 'criterion': 'gini'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search5.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  91.77%\n",
      "Valid Accuracy:  87.32%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {search5.best_estimator_.score(X1_train_sub, y1_train_sub)*100: 0.2f}%\") \n",
    "print(f\"Valid Accuracy: {search5.best_estimator_.score(X1_valid, y1_valid)*100: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 93.06%\n",
      "Valid Accuracy: 88.58%\n",
      "Test Accuracy: 89.18%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag1 = BaggingClassifier(base_estimator=search5.best_estimator_, \n",
    "                        n_estimators=150, \n",
    "                        oob_score=True, \n",
    "                        bootstrap=True,\n",
    "                        bootstrap_features=False, \n",
    "                        n_jobs=1, \n",
    "                        random_state=123)\n",
    "\n",
    "bag1.fit(X1_train_sub, y1_train_sub)\n",
    "print(f\"Train Accuracy: {bag1.score(X1_train_sub, y1_train_sub)*100:0.2f}%\")\n",
    "print(f\"Valid Accuracy: {bag1.score(X1_valid, y1_valid)*100:0.2f}%\")\n",
    "print(f\"Test Accuracy: {bag1.score(X1_test, y1_test)*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8912916961773778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 score: {f1_score(y1_test, bag1.predict(X1_test), average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "search6 = DecisionTreeClassifier(max_depth=18, \n",
    "                                 min_impurity_decrease=0.017492913234125385,\n",
    "                                 min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 90.99%\n",
      "Valid Accuracy: 90.30%\n",
      "Test Accuracy: 90.20%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag2 = BaggingClassifier(base_estimator=search6, \n",
    "                        n_estimators=50, \n",
    "                        oob_score=True, \n",
    "                        bootstrap=True,\n",
    "                        bootstrap_features=False, \n",
    "                        n_jobs=1, \n",
    "                        random_state=123)\n",
    "\n",
    "bag2.fit(X2_train_sub, y1_train_sub)\n",
    "print(f\"Train Accuracy: {bag2.score(X2_train_sub, y1_train_sub)*100:0.2f}%\")\n",
    "print(f\"Valid Accuracy: {bag2.score(X2_valid, y1_valid)*100:0.2f}%\")\n",
    "print(f\"Test Accuracy: {bag2.score(X2_test, y1_test)*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9018573316204965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 score: {f1_score(y1_test, bag2.predict(X2_test), average='weighted')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
